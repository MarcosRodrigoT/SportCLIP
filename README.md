# Textâ€‘Guided Sports Highlights <br>*(SportCLIPÂ â€” official code release)*

This repository contains the code, example data and reproducibility scripts for the paper  
**â€œTextâ€‘Guided Sports Highlights:Â A CLIPâ€‘Based Framework for Automatic VideoÂ Summarizationâ€**.

Our framework turns **any** sports video into a concise highlight reel by leveraging [OpenAI CLIP](https://github.com/openai/CLIP) imageâ€“text embeddings. The workflow is broken into three clear stages:

1. **Frame & embedding extraction** (`extractor.py`)
2. **Prompt engineering & event detection** (`multi_sentences.py`)
3. **Postâ€‘processing & metric computation** (`entire_pipeline.py`)

---

## Table of Contents

* [Installation](#installation)
* [Quickâ€‘start](#quick-start)
* [Directory Structure](#directory-structure)
* [Configuration](#configuration)
* [Outputs & Results](#outputs--results)
* [Citation](#citation)
* [License](#license)

---

## Installation

> Tested with **PythonÂ â‰¥Â 3.10** on Linux/macOS and with an NVIDIA GPU + CUDA (CPU also works, but slower).

```bash
# 1. Clone the repository
$ git clone https://github.com/<yourâ€‘username>/<repoâ€‘name>.git
$ cd <repoâ€‘name>

# 2. Create an isolated environment (recommended)
$ python3 -m venv venv
$ source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. Install Python dependencies
$ pip install --upgrade pip
$ pip install -r requirements.txt

# 4. Install and test FFmpeg / FFprobe (needed by extractor.py)
$ ffmpeg -version && ffprobe -version  # should print version info
```

---

## Quickâ€‘start

Assuming you have a video **`data/long_jump.mp4`** and its frameâ€‘level annotation file **`data/long_jump.csv`**:

```bash
# 1. Extract frames & CLIP embeddings (â‰ˆ realâ€‘time length, GPU makes this faster)
$ python extractor.py \
        --video data/long_jump.mp4 \
        --annotations data/long_jump.csv \
        --frames-per-second 29.97

# 2. Run multiple sentence prompts to discover the best highlight / nonâ€‘highlight pairs
$ python multi_sentences.py
# â–¸ results are written to results/<video_name>/

# 3. Generate final predictions, plots & evaluation metrics
$ python entire_pipeline.py
# â–¸ key outputs appear in results/<video_name>/Final result.png
```

All default hyperâ€‘parameters are hardâ€‘coded in the corresponding scripts and can be overridden via commandâ€‘line flags or by editing the classes at the top of each file.

---

## Directory Structure

```text
.
â”œâ”€â”€ extractor.py
â”œâ”€â”€ multi_sentences.py
â”œâ”€â”€ entire_pipeline.py
â”œâ”€â”€ utils.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ data/                 # â”€â–º videos (.mp4) & CSV annotations
â”œâ”€â”€ imgs/                 # â”€â–º one subâ€‘folder per video with extracted frames     (generated)
â”œâ”€â”€ img_embeddings/       # â”€â–º one subâ€‘folder per video with CLIP embeddings      (generated)
â””â”€â”€ results/              # â”€â–º perâ€‘video plots, logs & metrics                    (generated)
```

---

## Configuration

Most hyperâ€‘parameters live at the top of each script. &#x20;

* **Dataset layout**Â Â Place each `*.mp4` next to its `*.csv` annotation in the `data/` folder.
  The CSV must contain at least the columns `First frame`, `Last frame` and `Event type`.
* **GPU vs CPU**Â Â All scripts will automatically switch to CUDA if available.
* **Custom prompts**Â Â Edit the `highlight_sentences` and `not_highlight_sentences` lists in `multi_sentences.py`.

---

## Outputs & Results

After running the full pipeline you will find:

| Path                               | Description                                       |
| ---------------------------------- | ------------------------------------------------- |
| `results/<video>/Final result.png` | Summary plot of ground truth vs predictions       |
| `results/<video>/Pairs used.txt`   | List of prompt pairs kept after filtering         |
| `results/<video>/*.pkl`            | Pickled KDE curves, scores & auxiliary stats      |
| `results/<video>/*.png`            | Histograms and stitched diagnostic images         |

---

## Citation

If you find our work useful in your research, please cite:

```bibtex
@article{rodrigo2025sportclip,
  title   = {Text-Guided Sports Highlights: A CLIP-Based Framework for Automatic Video Summarization},
  author  = {Marcos Rodrigo, Carlos Cuevas, and Narciso GarcÃ­a},
  journal = {IEEE Transactions on Consumer Electronics},
  year    = {2025}
}
```

---

## License

This project is released under the **MIT License**. &#x20;
See [`LICENSE`](LICENSE) for details.

Happy summarizing! ğŸ¬ğŸ…